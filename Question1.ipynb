{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisa2000/ML_HW2/blob/main/Question1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p03hoMY_KC5B"
      },
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Sajjad Amini</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2023</h4>\n",
        "\n",
        "**Student Name**:\n",
        "\n",
        "**Student ID**:\n",
        "\n",
        "# Linear Regression\n",
        "\n",
        "In this exercise, we want to examine **linear regression**. For this purpose, we have prepared a dataset in the `q1.csv` file. This dataset is used to estimate the **heating load** and **cooling load** of a building based on its parameters. The parameters in this dataset are explained below:\n",
        "\n",
        "- $X_1$: Relative Compactness\n",
        "- $X_2$: Surface Area\n",
        "- $X_3$: Wall Area\n",
        "- $X_4$: Roof Area\n",
        "- $X_5$: Overall Height\n",
        "- $X_6$: Orientation\n",
        "- $X_7$: Glazing Area\n",
        "- $X_8$: Glazing Area Distribution\n",
        "- $Y_1$: Heating Load\n",
        "- $Y_2$: Cooling Load\n",
        "\n",
        "**Note**: For the sake of simplicity, we will only focus on estimating the **heating load** in this problem. Also, please note that we have some inline questions in this notebook, for which you should write your answers in the **Answer** section below each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6_GSec1OXRM"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "First we import libraries that we need for this assignment.\n",
        "\n",
        "**Attention**: You should only use these libraries. Other libraries are not acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5u7kqX0wONrr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9he9c_YvMqAL"
      },
      "source": [
        "## Reading Data and Preprocessing\n",
        "\n",
        "In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.\n",
        "\n",
        "First, we read the data in the cell below and extract an $m \\times n$ matrix, $X$, and an $m \\times 1$ vector, $Y$, from it, which represent our knowledge about the building (`X1`, `X2`, ..., `X8`) and heating load (`Y1`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bV10SRSaJ_DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d50505d-c381-4508-a731-5cae944ba3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
            "0    0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
            "1    0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
            "2    0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
            "3    0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
            "4    0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n",
            "..    ...    ...    ...     ...  ...  ..  ...  ..    ...    ...\n",
            "763  0.64  784.0  343.0  220.50  3.5   5  0.4   5  17.88  21.40\n",
            "764  0.62  808.5  367.5  220.50  3.5   2  0.4   5  16.54  16.88\n",
            "765  0.62  808.5  367.5  220.50  3.5   3  0.4   5  16.44  17.11\n",
            "766  0.62  808.5  367.5  220.50  3.5   4  0.4   5  16.48  16.61\n",
            "767  0.62  808.5  367.5  220.50  3.5   5  0.4   5  16.64  16.03\n",
            "\n",
            "[768 rows x 10 columns]\n",
            "       X1     X2     X3      X4   X5  X6   X7  X8\n",
            "0    0.98  514.5  294.0  110.25  7.0   2  0.0   0\n",
            "1    0.98  514.5  294.0  110.25  7.0   3  0.0   0\n",
            "2    0.98  514.5  294.0  110.25  7.0   4  0.0   0\n",
            "3    0.98  514.5  294.0  110.25  7.0   5  0.0   0\n",
            "4    0.90  563.5  318.5  122.50  7.0   2  0.0   0\n",
            "..    ...    ...    ...     ...  ...  ..  ...  ..\n",
            "763  0.64  784.0  343.0  220.50  3.5   5  0.4   5\n",
            "764  0.62  808.5  367.5  220.50  3.5   2  0.4   5\n",
            "765  0.62  808.5  367.5  220.50  3.5   3  0.4   5\n",
            "766  0.62  808.5  367.5  220.50  3.5   4  0.4   5\n",
            "767  0.62  808.5  367.5  220.50  3.5   5  0.4   5\n",
            "\n",
            "[768 rows x 8 columns]\n",
            "        Y1\n",
            "0    15.55\n",
            "1    15.55\n",
            "2    15.55\n",
            "3    15.55\n",
            "4    20.84\n",
            "..     ...\n",
            "763  17.88\n",
            "764  16.54\n",
            "765  16.44\n",
            "766  16.48\n",
            "767  16.64\n",
            "\n",
            "[768 rows x 1 columns]\n",
            "(768, 8)\n",
            "(768, 1)\n"
          ]
        }
      ],
      "source": [
        "X, Y = None, None\n",
        "\n",
        "### START CODE HERE ###\n",
        "df=pd.read_csv('q1.csv')\n",
        "print(df)\n",
        "features=['X1','X2','X3','X4','X5','X6','X7','X8']\n",
        "X=df[features]\n",
        "features1=['Y1']\n",
        "Y=df[features1]\n",
        "print(X)\n",
        "print(Y)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZeww-2OAuX"
      },
      "source": [
        "Next, we should normalize our data. For normalizing a vector $\\mathbf{x}$, a very common method is to use this formula:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{norm} = \\dfrac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}}\n",
        "$$\n",
        "\n",
        "Here, $\\overline{x}$ and $\\sigma_\\mathbf{x}$ denote the mean and standard deviation of vector $\\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.\n",
        "\n",
        "**Question**: Briefly explain why we need to normalize our data before starting the training.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VrfjRu-IMPPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y2i0bjxUPak2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85644c25-5180-416b-fe38-99dc53c40daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           X1        X2        X3        X4        X5        X6        X7  \\\n",
            "0    0.000046  0.024057  0.013747  0.005155  0.000327  0.000094  0.000000   \n",
            "1    0.000046  0.024057  0.013747  0.005155  0.000327  0.000140  0.000000   \n",
            "2    0.000046  0.024057  0.013747  0.005155  0.000327  0.000187  0.000000   \n",
            "3    0.000046  0.024057  0.013747  0.005155  0.000327  0.000234  0.000000   \n",
            "4    0.000042  0.026348  0.014892  0.005728  0.000327  0.000094  0.000000   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "763  0.000030  0.036658  0.016038  0.010310  0.000164  0.000234  0.000019   \n",
            "764  0.000029  0.037804  0.017184  0.010310  0.000164  0.000094  0.000019   \n",
            "765  0.000029  0.037804  0.017184  0.010310  0.000164  0.000140  0.000019   \n",
            "766  0.000029  0.037804  0.017184  0.010310  0.000164  0.000187  0.000019   \n",
            "767  0.000029  0.037804  0.017184  0.010310  0.000164  0.000234  0.000019   \n",
            "\n",
            "           X8  \n",
            "0    0.000000  \n",
            "1    0.000000  \n",
            "2    0.000000  \n",
            "3    0.000000  \n",
            "4    0.000000  \n",
            "..        ...  \n",
            "763  0.000234  \n",
            "764  0.000234  \n",
            "765  0.000234  \n",
            "766  0.000234  \n",
            "767  0.000234  \n",
            "\n",
            "[768 rows x 8 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from pandas._libs.tslibs import normalize_i8_timestamps\n",
        "### START CODE HERE ###\n",
        "x_m=np.mean(X)\n",
        "x_sigma=np.var(X)\n",
        "xn1=np.linalg.norm(X)\n",
        "yn1=np.linalg.norm(X)\n",
        "xn2=X/xn1\n",
        "yn2=Y/yn1\n",
        "print(xn2)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmHQ-mxQzDE"
      },
      "source": [
        "Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \\times (n+1)$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLfV7VQNRCfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35209dd4-06c8-4a4e-c301-2219ba6c37e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.00000000e+00 4.58226975e-05 2.40569162e-02 ... 9.35157092e-05\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 4.58226975e-05 2.40569162e-02 ... 1.40273564e-04\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 4.58226975e-05 2.40569162e-02 ... 1.87031418e-04\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.00000000e+00 2.89898699e-05 3.78037254e-02 ... 1.40273564e-04\n",
            "  1.87031418e-05 2.33789273e-04]\n",
            " [1.00000000e+00 2.89898699e-05 3.78037254e-02 ... 1.87031418e-04\n",
            "  1.87031418e-05 2.33789273e-04]\n",
            " [1.00000000e+00 2.89898699e-05 3.78037254e-02 ... 2.33789273e-04\n",
            "  1.87031418e-05 2.33789273e-04]]\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "m=xn2.shape[0]\n",
        "x1=np.c_[np.ones((m,1)),xn2]\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO--dppbRsdb"
      },
      "source": [
        "## Training Model Using Direct Method\n",
        "\n",
        "We know that the loss function in linear regression is defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\mathbf{w}^\\top\\mathbf{x}_i-y_i)^2\n",
        "$$\n",
        "\n",
        "Here, $w$ is the weight vector and $(x_i, y_i)$ represents the $i$th data point. First, write a function that takes $X$, $Y$, and $w$ as inputs and returns the loss value in the next cell. Note that your implementation should be fully vectorized, meaning that you are not allowed to use any loops in your function and should only use functions prepared in the numpy library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bSbXtyXCRzD3"
      },
      "outputs": [],
      "source": [
        "def loss(X, Y, w):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  w: an (n+1) by 1 weight vector\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  loss = None\n",
        "  ### START CODE HERE ###\n",
        "  yy=np.dot(X,w)\n",
        "  loss=np.sum(np.dot((yy-Y).T,(yy-Y)))/np.size(yy)\n",
        "  ### END CODE HERE ###\n",
        "  return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuhSB9zaTfwm"
      },
      "source": [
        "Now, we want to calculate the weight matrix, $w$, using the direct method. By direct method, we mean finding the answer to the optimization problem below directly using linear algebra, without using iterative methods:\n",
        "\n",
        "$$\n",
        "\\min_{w} \\mathcal{L}(w)\n",
        "$$\n",
        "\n",
        "Question: What is the answer to this problem in terms of $X$ and $Y$?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Now you should implement a function that receives $X$ and $Y$ as input and returns $w$. Note that your implementation should also be fully vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-gCUK01DT-cW"
      },
      "outputs": [],
      "source": [
        "def direct_method(X, Y):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  '''\n",
        "  w = None\n",
        "  m,n=X.shape\n",
        "  Y=np.reshape(Y,(m,1))\n",
        "  ### START CODE HERE ###\n",
        "  w=np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,Y))\n",
        "  ### END CODE HERE ###\n",
        "  return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFIeOaSUvlw"
      },
      "source": [
        "Finally, we want to evaluate our loss for this problem. Run the cell below to calculate the loss of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6gGDh11VU8vF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36db1fb9-ae0a-4365-b2a5-eb5f005965c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for this problem using direct method is 2.2484819267723002e-08\n"
          ]
        }
      ],
      "source": [
        "w = direct_method(x1, yn2) # calculating w using direct method\n",
        "\n",
        "print(f\"loss for this problem using direct method is {loss(x1, yn2, w)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaG14YG9VXiS"
      },
      "source": [
        "## Training Model Using Gradient Descent\n",
        "\n",
        "Now, instead of using the direct method to calculate $w$, we want to use the **Gradient Descent** algorithm. We know that in this algorithm, in each iteration, we should update our weight vector with:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{w}^{(t)})\n",
        "$$\n",
        "\n",
        "Here, $w^{t}$ represents the weight matrix in the $t$th iteration, and $\\alpha$ represents the learning rate.\n",
        "\n",
        "**Question**: Write an expression for $\\nabla\\mathcal{L}(\\mathbf{w})$.\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "Now, write a function that computes the gradient of $\\mathcal{L}(\\mathbf{w})$. This function should receive $X$, $Y$, and $\\mathbf{w}$ as inputs and return an $(n+1) \\times 1$ vector, which represents $\\nabla\\mathcal{L}(\\mathbf{w})$. Note that your implementation should also be **fully vectorized**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yqVQ-8I-VeVc"
      },
      "outputs": [],
      "source": [
        "def gradient(X, Y, w):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  w: an (n+1) by 1 weight vector\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  grad = None\n",
        "  ### START CODE HERE ###\n",
        "  yy=np.dot(X,w)\n",
        "  grad=2*X*(yy-Y)\n",
        "  ### END CODE HERE ###\n",
        "  return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJP5KaZzXz5K"
      },
      "source": [
        "Now, we are ready to implement the Gradient Descent algorithm. Complete the function below for this purpose. Note that this function receives $X$, $Y$, the learning rate, and the number of iterations as inputs. This function should return two parameters. The first parameter is $\\mathbf{w}$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code. Also note that you should initialize $\\mathbf{w}$ with the `randn` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdfNjz5DYgD7"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y, alpha, num_iter):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  alpha: learning rate\n",
        "  num_iter: number of iterations of the algorithm\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  w, loss_history = None, None \n",
        "  ### START CODE HERE ###\n",
        "  w=np.ones(n)\n",
        "  for i in range(num_iter):\n",
        "    yy=np.dot(X,w)\n",
        "    w=w-(1/m)*alpha*(np.dot(X.T,(yy-Y)))\n",
        "    loss=(1/2*m)*np.sum(np.square(yy-Y))\n",
        "    loss_history.append(loss)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "  return w, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjGioRweZK9O"
      },
      "source": [
        "Now, run the `gradient_descent` function for 5 different values of the learning rate. Plot the `loss_history` of these 5 different values in the same figure.\n",
        "\n",
        "**Question**: Discuss the effect of the learning rate and find the best value of this parameter.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yMuwbOokZtcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "e0c37ab7-a030-44fa-8b55-ffd053ca08e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e05ca5b10954>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### START CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mj1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mj2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab:blue'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-69cdd842b7f8>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, alpha, num_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0myy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     ):\n\u001b[0;32m-> 2113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marraylike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# for binary ops, use our custom dunder methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_dispatch_ufunc_to_dunder_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/ops_dispatch.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__mul__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7590\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_prepare_scalar_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7592\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7594\u001b[0m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36malign_method_FRAME\u001b[0;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[0;34m(right)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    240\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 768"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "j1,k1=gradient_descent(x1,yn2,0.001,1000)\n",
        "j2,k2=gradient_descent(x1,yn2,0.005,1000)\n",
        "color='tab:blue'\n",
        "fig,ax1=plt.subplots()\n",
        "ax1.plot(k1,label='$\\\\alpha_{0.001}$',linestyle='--',color=color)\n",
        "ax1.plot(k2,label='$\\\\alpha_{0.001}$',linestyle='--',color=color)\n",
        "ax1.set_title('value of loss over iteration')\n",
        "ax1.set_xlabel('iteration')\n",
        "ax1.set_ylabel('$\\\\alpha$',color=color)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lyJhZqZ18d"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Compare the answer of two different methods that we used earlier.\n",
        "\n",
        "**Question**: Discuss these two methods and compare them with each other. When is it better to use the direct method, and when is it better to use Gradient Descent?\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF15dsmXaUzJ"
      },
      "source": [
        "## (Additional Part) Stochastic Gradient Descent\n",
        "\n",
        "When the number of data points becomes large, calculating the gradient becomes very complicated. In these circumstances, we use **Stochastic Gradient Descent**. In this algorithm, instead of using all of the data points to calculate the gradient, we use only a small number of them. We choose these small number of points randomly in each iteration. Implement this algorithm, and use it to calculate $w$, and then compare the result with the preceding parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6cHTQFgOaQBB"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, Y, k, alpha, num_iter):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  k: number of data points used in each iteration\n",
        "  alpha: learning rate\n",
        "  num_iter: number of iterations of the algorithm\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  w, loss_history = None, None \n",
        "  ### START CODE HERE ###\n",
        "  w=np.ones(n)\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    batch=np.random.randint(m)\n",
        "    x_batch=X[batch,:]\n",
        "    y_batch=Y[batch,:]\n",
        "    prediction=np.dot(x_batch,w.T)\n",
        "    loss_history.append((1/2*m)*np.sum(np.square(prediction-y_batch)))\n",
        "    gradient=x_batch.T.dot(prediction-y_batch)\n",
        "    update=-alpha*gradient\n",
        "    w=w+update\n",
        "  ### END CODE HERE ###\n",
        "  return w, loss_history"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}