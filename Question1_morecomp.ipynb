{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisa2000/ML_HW2/blob/main/Question1_morecomp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p03hoMY_KC5B"
      },
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Sajjad Amini</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2023</h4>\n",
        "\n",
        "**Student Name**:\n",
        "\n",
        "**Student ID**:\n",
        "\n",
        "# Linear Regression\n",
        "\n",
        "In this exercise, we want to examine **linear regression**. For this purpose, we have prepared a dataset in the `q1.csv` file. This dataset is used to estimate the **heating load** and **cooling load** of a building based on its parameters. The parameters in this dataset are explained below:\n",
        "\n",
        "- $X_1$: Relative Compactness\n",
        "- $X_2$: Surface Area\n",
        "- $X_3$: Wall Area\n",
        "- $X_4$: Roof Area\n",
        "- $X_5$: Overall Height\n",
        "- $X_6$: Orientation\n",
        "- $X_7$: Glazing Area\n",
        "- $X_8$: Glazing Area Distribution\n",
        "- $Y_1$: Heating Load\n",
        "- $Y_2$: Cooling Load\n",
        "\n",
        "**Note**: For the sake of simplicity, we will only focus on estimating the **heating load** in this problem. Also, please note that we have some inline questions in this notebook, for which you should write your answers in the **Answer** section below each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6_GSec1OXRM"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "First we import libraries that we need for this assignment.\n",
        "\n",
        "**Attention**: You should only use these libraries. Other libraries are not acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5u7kqX0wONrr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9he9c_YvMqAL"
      },
      "source": [
        "## Reading Data and Preprocessing\n",
        "\n",
        "In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.\n",
        "\n",
        "First, we read the data in the cell below and extract an $m \\times n$ matrix, $X$, and an $m \\times 1$ vector, $Y$, from it, which represent our knowledge about the building (`X1`, `X2`, ..., `X8`) and heating load (`Y1`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bV10SRSaJ_DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331d1c2f-28ec-4be9-9fad-6a2824a56aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
            "0    0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
            "1    0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
            "2    0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
            "3    0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
            "4    0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n",
            "..    ...    ...    ...     ...  ...  ..  ...  ..    ...    ...\n",
            "763  0.64  784.0  343.0  220.50  3.5   5  0.4   5  17.88  21.40\n",
            "764  0.62  808.5  367.5  220.50  3.5   2  0.4   5  16.54  16.88\n",
            "765  0.62  808.5  367.5  220.50  3.5   3  0.4   5  16.44  17.11\n",
            "766  0.62  808.5  367.5  220.50  3.5   4  0.4   5  16.48  16.61\n",
            "767  0.62  808.5  367.5  220.50  3.5   5  0.4   5  16.64  16.03\n",
            "\n",
            "[768 rows x 10 columns]\n",
            "       X1     X2     X3      X4   X5  X6   X7  X8\n",
            "0    0.98  514.5  294.0  110.25  7.0   2  0.0   0\n",
            "1    0.98  514.5  294.0  110.25  7.0   3  0.0   0\n",
            "2    0.98  514.5  294.0  110.25  7.0   4  0.0   0\n",
            "3    0.98  514.5  294.0  110.25  7.0   5  0.0   0\n",
            "4    0.90  563.5  318.5  122.50  7.0   2  0.0   0\n",
            "..    ...    ...    ...     ...  ...  ..  ...  ..\n",
            "763  0.64  784.0  343.0  220.50  3.5   5  0.4   5\n",
            "764  0.62  808.5  367.5  220.50  3.5   2  0.4   5\n",
            "765  0.62  808.5  367.5  220.50  3.5   3  0.4   5\n",
            "766  0.62  808.5  367.5  220.50  3.5   4  0.4   5\n",
            "767  0.62  808.5  367.5  220.50  3.5   5  0.4   5\n",
            "\n",
            "[768 rows x 8 columns]\n",
            "        Y1\n",
            "0    15.55\n",
            "1    15.55\n",
            "2    15.55\n",
            "3    15.55\n",
            "4    20.84\n",
            "..     ...\n",
            "763  17.88\n",
            "764  16.54\n",
            "765  16.44\n",
            "766  16.48\n",
            "767  16.64\n",
            "\n",
            "[768 rows x 1 columns]\n",
            "(768, 8)\n",
            "(768, 1)\n"
          ]
        }
      ],
      "source": [
        "X, Y = None, None\n",
        "\n",
        "### START CODE HERE ###\n",
        "df=pd.read_csv('q1.csv')\n",
        "print(df)\n",
        "features=['X1','X2','X3','X4','X5','X6','X7','X8']\n",
        "X=df[features]\n",
        "features1=['Y1']\n",
        "Y=df[features1]\n",
        "print(X)\n",
        "print(Y)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZeww-2OAuX"
      },
      "source": [
        "Next, we should normalize our data. For normalizing a vector $\\mathbf{x}$, a very common method is to use this formula:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{norm} = \\dfrac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}}\n",
        "$$\n",
        "\n",
        "Here, $\\overline{x}$ and $\\sigma_\\mathbf{x}$ denote the mean and standard deviation of vector $\\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.\n",
        "\n",
        "**Question**: Briefly explain why we need to normalize our data before starting the training.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.\n",
        "To overcome the model learning problem, we normalize the data. We make sure that the different features take on similar ranges of values so that gradient descents can converge more quickly"
      ],
      "metadata": {
        "id": "VrfjRu-IMPPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Y2i0bjxUPak2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3658f196-9946-48bb-b83e-0c8a38657ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           X1        X2        X3        X4   X5        X6        X7        X8\n",
            "0    2.041777 -1.785875 -0.561951 -1.470077  1.0 -1.341641 -1.760447 -1.814575\n",
            "1    2.041777 -1.785875 -0.561951 -1.470077  1.0 -0.447214 -1.760447 -1.814575\n",
            "2    2.041777 -1.785875 -0.561951 -1.470077  1.0  0.447214 -1.760447 -1.814575\n",
            "3    2.041777 -1.785875 -0.561951 -1.470077  1.0  1.341641 -1.760447 -1.814575\n",
            "4    1.284979 -1.229239  0.000000 -1.198678  1.0 -1.341641 -1.760447 -1.814575\n",
            "..        ...       ...       ...       ...  ...       ...       ...       ...\n",
            "763 -1.174613  1.275625  0.561951  0.972512 -1.0  1.341641  1.244049  1.411336\n",
            "764 -1.363812  1.553943  1.123903  0.972512 -1.0 -1.341641  1.244049  1.411336\n",
            "765 -1.363812  1.553943  1.123903  0.972512 -1.0 -0.447214  1.244049  1.411336\n",
            "766 -1.363812  1.553943  1.123903  0.972512 -1.0  0.447214  1.244049  1.411336\n",
            "767 -1.363812  1.553943  1.123903  0.972512 -1.0  1.341641  1.244049  1.411336\n",
            "\n",
            "[768 rows x 8 columns]\n",
            "           Y1\n",
            "0   -0.670116\n",
            "1   -0.670116\n",
            "2   -0.670116\n",
            "3   -0.670116\n",
            "4   -0.145503\n",
            "..        ...\n",
            "763 -0.439049\n",
            "764 -0.571937\n",
            "765 -0.581854\n",
            "766 -0.577887\n",
            "767 -0.562020\n",
            "\n",
            "[768 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from pandas._libs.tslibs import normalize_i8_timestamps\n",
        "### START CODE HERE ###\n",
        "x_m=np.mean(X)\n",
        "x_sigma=np.std(X)\n",
        "xn1=np.linalg.norm(X)\n",
        "yn1=np.linalg.norm(X)\n",
        "xn2=X/xn1\n",
        "yn2=Y/yn1\n",
        "g=(Y-np.mean(Y))/np.std(Y)\n",
        "l=(X-x_m)/x_sigma\n",
        "print(l)\n",
        "print(g)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmHQ-mxQzDE"
      },
      "source": [
        "Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \\times (n+1)$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLfV7VQNRCfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f523bafd-c760-4baa-e168-3e44f0d3b08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.          2.04177671 -1.78587489 ... -1.34164079 -1.76044698\n",
            "  -1.81457514]\n",
            " [ 1.          2.04177671 -1.78587489 ... -0.4472136  -1.76044698\n",
            "  -1.81457514]\n",
            " [ 1.          2.04177671 -1.78587489 ...  0.4472136  -1.76044698\n",
            "  -1.81457514]\n",
            " ...\n",
            " [ 1.         -1.36381225  1.55394308 ... -0.4472136   1.2440492\n",
            "   1.41133622]\n",
            " [ 1.         -1.36381225  1.55394308 ...  0.4472136   1.2440492\n",
            "   1.41133622]\n",
            " [ 1.         -1.36381225  1.55394308 ...  1.34164079  1.2440492\n",
            "   1.41133622]]\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "m=l.shape[0]\n",
        "x1=np.c_[np.ones((m,1)),l]\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO--dppbRsdb"
      },
      "source": [
        "## Training Model Using Direct Method\n",
        "\n",
        "We know that the loss function in linear regression is defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\mathbf{w}^\\top\\mathbf{x}_i-y_i)^2\n",
        "$$\n",
        "\n",
        "Here, $w$ is the weight vector and $(x_i, y_i)$ represents the $i$th data point. First, write a function that takes $X$, $Y$, and $w$ as inputs and returns the loss value in the next cell. Note that your implementation should be fully vectorized, meaning that you are not allowed to use any loops in your function and should only use functions prepared in the numpy library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bSbXtyXCRzD3"
      },
      "outputs": [],
      "source": [
        "def loss(X, Y, w):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  w: an (n+1) by 1 weight vector\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  loss = None\n",
        "  ### START CODE HERE ###\n",
        "  yy=np.dot(X,w)\n",
        "  loss=np.sum(np.dot((yy-Y).T,(yy-Y)))/np.size(yy)\n",
        "  ### END CODE HERE ###\n",
        "  return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuhSB9zaTfwm"
      },
      "source": [
        "Now, we want to calculate the weight matrix, $w$, using the direct method. By direct method, we mean finding the answer to the optimization problem below directly using linear algebra, without using iterative methods:\n",
        "\n",
        "$$\n",
        "\\min_{w} \\mathcal{L}(w)\n",
        "$$\n",
        "\n",
        "Question: What is the answer to this problem in terms of $X$ and $Y$?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Now you should implement a function that receives $X$ and $Y$ as input and returns $w$. Note that your implementation should also be fully vectorized."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the math of loss function :\n",
        "linear eqution:z=Xw+b , \n",
        "prediction:y_hat=z , \n",
        "loss_function:0.5(y_hat-Y)^2\n",
        ".We are interested in calculating the derivative of the loss with respect to \n",
        "z\n"
      ],
      "metadata": {
        "id": "dqDqMj2rHDKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-gCUK01DT-cW"
      },
      "outputs": [],
      "source": [
        "def direct_method(X, Y):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  '''\n",
        "  w = None\n",
        "  m,n=X.shape\n",
        "  Y=np.reshape(Y,(m,1))\n",
        "  ### START CODE HERE ###\n",
        "  w=np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,Y))\n",
        "  ### END CODE HERE ###\n",
        "  return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFIeOaSUvlw"
      },
      "source": [
        "Finally, we want to evaluate our loss for this problem. Run the cell below to calculate the loss of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6gGDh11VU8vF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843cadb5-8c89-49da-a2e8-15df13e0074f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for this problem using direct method is 0.09523825689116268\n"
          ]
        }
      ],
      "source": [
        "w = direct_method(l, g) # calculating w using direct method\n",
        "\n",
        "print(f\"loss for this problem using direct method is {loss(l, g, w)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaG14YG9VXiS"
      },
      "source": [
        "## Training Model Using Gradient Descent\n",
        "\n",
        "Now, instead of using the direct method to calculate $w$, we want to use the **Gradient Descent** algorithm. We know that in this algorithm, in each iteration, we should update our weight vector with:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{w}^{(t)})\n",
        "$$\n",
        "\n",
        "Here, $w^{t}$ represents the weight matrix in the $t$th iteration, and $\\alpha$ represents the learning rate.\n",
        "\n",
        "**Question**: Write an expression for $\\nabla\\mathcal{L}(\\mathbf{w})$.\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "Now, write a function that computes the gradient of $\\mathcal{L}(\\mathbf{w})$. This function should receive $X$, $Y$, and $\\mathbf{w}$ as inputs and return an $(n+1) \\times 1$ vector, which represents $\\nabla\\mathcal{L}(\\mathbf{w})$. Note that your implementation should also be **fully vectorized**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mhLDYyOVHI0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yqVQ-8I-VeVc"
      },
      "outputs": [],
      "source": [
        "def gradient(X, Y, w):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  w: an (n+1) by 1 weight vector\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  grad = None\n",
        "  ### START CODE HERE ###\n",
        "  yy=np.dot(X,w)\n",
        "  grad=2*X*(yy-Y)\n",
        "  ### END CODE HERE ###\n",
        "  return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJP5KaZzXz5K"
      },
      "source": [
        "Now, we are ready to implement the Gradient Descent algorithm. Complete the function below for this purpose. Note that this function receives $X$, $Y$, the learning rate, and the number of iterations as inputs. This function should return two parameters. The first parameter is $\\mathbf{w}$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code. Also note that you should initialize $\\mathbf{w}$ with the `randn` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdfNjz5DYgD7"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y, alpha, num_iter):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  alpha: learning rate\n",
        "  num_iter: number of iterations of the algorithm\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  w = None\n",
        "  loss_history=[]\n",
        "  ### START CODE HERE ###\n",
        "  w=np.random.rand(n)\n",
        "  for i in range(num_iter):\n",
        "    yy=np.dot(X,w)\n",
        "    yy=np.reshape(yy,(m,1))\n",
        "    i=np.reshape(((1/m)*0.01*(np.dot(l.T,(yy-g)))),(n))\n",
        "    w=w-i\n",
        "    loss=(1/2*m)*np.sum(np.square(yy-Y))\n",
        "    loss_history.append(loss)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "  return w, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjGioRweZK9O"
      },
      "source": [
        "Now, run the `gradient_descent` function for 5 different values of the learning rate. Plot the `loss_history` of these 5 different values in the same figure.\n",
        "\n",
        "**Question**: Discuss the effect of the learning rate and find the best value of this parameter.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate is used to scale the magnitude of parameter updates during gradient descent. The choice of the value for learning rate can impact two things: 1) how fast the algorithm learns and 2) whether the cost function is minimized or not.It can be seen that for an optimal value of the learning rate, the cost function value is minimized in a few iterations (smaller time).\n",
        "If the learning rate used is lower than the optimal value, the number of iterations/epochs required to minimize the cost function is high (takes longer time).If the learning rate is high, the cost function could saturate at a value higher than the minimum value. If the learning rate selected is very high, the cost function could continue to increase with iterations/epochs. An optimal learning rate is not easy to find for a given problem."
      ],
      "metadata": {
        "id": "hVcZoev2HLHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yMuwbOokZtcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "0380320f-4bd1-4483-b286-30c4cb81eb49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$\\\\alpha$')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "j1,k1=gradient_descent(l,g,0.001,1000)\n",
        "j2,k2=gradient_descent(l,g,0.005,1000)\n",
        "j3,k3=gradient_descent(l,g,0.01,1000)\n",
        "j4,k4=gradient_descent(l,g,0.1,1000)\n",
        "j5,k5=gradient_descent(l,g,1,1000)\n",
        "color='tab:blue'\n",
        "fig,ax1=plt.subplots()\n",
        "ax1.plot(k1,label='$\\\\alpha_{0.001}$',linestyle='-',color='blue')\n",
        "ax1.plot(k2,label='$\\\\alpha_{0.001}$',linestyle='-',color='black')\n",
        "ax1.plot(k3,label='$\\\\alpha_{0.001}$',linestyle='-',color='yellow')\n",
        "ax1.plot(k4,label='$\\\\alpha_{0.001}$',linestyle='-',color='red')\n",
        "ax1.plot(k5,label='$\\\\alpha_{0.001}$',linestyle='-',color='green')\n",
        "ax1.set_title('value of loss over iteration')\n",
        "ax1.set_xlabel('iteration')\n",
        "ax1.set_ylabel('$\\\\alpha$',color=color)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i think alpha=0.01 is the best value for this problem."
      ],
      "metadata": {
        "id": "kaxAZcyj5icD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lyJhZqZ18d"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Compare the answer of two different methods that we used earlier.\n",
        "\n",
        "**Question**: Discuss these two methods and compare them with each other. When is it better to use the direct method, and when is it better to use Gradient Descent?\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in some cases.\n",
        "The direct method looks very simple,even computationally,because it only works for univariate case, i.e. when we have only one variable. In the multivariate case, when we have many variables,the formula is slightly more complicated on paper and requires much more calculations.\n",
        "the gradient descent allows to save a lot of time on calculations. Moreover, the way it's done allows for a trivial parallelization, i.e. distributing the calculations across multiple processors or machines. The linear algebra solution can also be parallelized but it's more complicated and still expensive.Additionally, there are versions of gradient descent when we keep only a piece of our data in memory, lowering the requirements for computer memory. Overall, for extra large problems it's more efficient than linear algebra solution.\n",
        "\n",
        "This becomes even more important as the dimensionality increases, when you have thousands of variables like in machine learning.\n",
        "One other reason is that gradient descent is more of a general method. For many machine learning problems, the cost function is not convex (e.g., matrix factorization, neural networks) so we cannot use a closed form solution. In those cases, gradient descent is used to find some good local optimum points."
      ],
      "metadata": {
        "id": "b-jnqeL8HONL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF15dsmXaUzJ"
      },
      "source": [
        "## (Additional Part) Stochastic Gradient Descent\n",
        "\n",
        "When the number of data points becomes large, calculating the gradient becomes very complicated. In these circumstances, we use **Stochastic Gradient Descent**. In this algorithm, instead of using all of the data points to calculate the gradient, we use only a small number of them. We choose these small number of points randomly in each iteration. Implement this algorithm, and use it to calculate $w$, and then compare the result with the preceding parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6cHTQFgOaQBB"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, Y, k, alpha, num_iter):\n",
        "  '''\n",
        "  X: an m by (n+1) matrix which includes inputs\n",
        "  Y: an m by 1 vector which includes heating loads\n",
        "  k: number of data points used in each iteration\n",
        "  alpha: learning rate\n",
        "  num_iter: number of iterations of the algorithm\n",
        "  '''\n",
        "  m, n = X.shape\n",
        "  w = None\n",
        "  loss_history=[]\n",
        "  ### START CODE HERE ###\n",
        "  w=np.ones(n)\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    batch=np.random.randint(m)\n",
        "    x_batch=X[batch,:]\n",
        "    y_batch=Y[batch,:]\n",
        "    prediction=np.dot(x_batch,w.T)\n",
        "    loss_history.append((1/2*m)*np.sum(np.square(prediction-y_batch)))\n",
        "    gradient=x_batch.T.dot(prediction-y_batch)\n",
        "    update=-alpha*gradient\n",
        "    w=w+update\n",
        "  ### END CODE HERE ###\n",
        "  return w, loss_history\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}